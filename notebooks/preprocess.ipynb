{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In the previous session, you successfully acquired and prepared a subset of the ESS10 data \n",
    "\n",
    "Keep in minde we aim to the following hypothesis:\n",
    "\n",
    "> **Centrist individuals exhibit lower levels of affective polarization.**\n",
    "\n",
    "Today, you will clean this dataset to make it analysis-ready. This involves:\n",
    "\n",
    "1. Loading the subset data you created last time\n",
    "2. Filtering observations (focusing on France)\n",
    "3. Recoding variables for analysis\n",
    "4. Creating relevant variables for the analysis\n",
    "5. Saving the cleaned dataset\n",
    "\n",
    "### Tips\n",
    "\n",
    "- You should adapt code from previous [notebooks](https://github.com/mickaeltemporao/materials/tree/main), such as:\n",
    "    - `05-data-exploration-rows.ipynb`\n",
    "    - `06-data-management-existing-values.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-intro",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We have prepared a helper function in the `code/preprocess.py` module that will load your subset data. If the subset file doesn't exist, it will recreate it from the raw ESS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Adding the code directory to path\n",
    "sys.path.append('../code')  \n",
    "\n",
    "# Import the preprocessing module\n",
    "from preprocess import subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subset data using our helper function\n",
    "df = subset()\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68508097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check before we get started\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-intro",
   "metadata": {},
   "source": [
    "# Let's Start!\n",
    "## Filtering French Observations\n",
    "\n",
    "Since some variables are country-specific, let's filter our dataset to focus on French respondents.\n",
    "\n",
    "**Task:**\n",
    "1. Filter the dataset to include only respondents from France (`country == 'FR'`)\n",
    "2. Create a new dataframe called `df_france`\n",
    "3. Drop the `cntry` column\n",
    "4. Display the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recode-intro",
   "metadata": {},
   "source": [
    "## Filtering Relevant Observations\n",
    "\n",
    "Before we can analyze the data, we need to understand how the variables are coded. \n",
    "\n",
    "If you check the codebook, we see that most variables have values that are not applicable to our analysis (66, 77, 88, ...). \n",
    "For now, we will simply remove irrelevant observations.\n",
    "\n",
    "> The codebook has been automatically downloaded in `data/raw/ESS10 codebook.html`\n",
    "\n",
    "**Task:**\n",
    "1. Filter each variable to include only relevant observations\n",
    "2. Check the values of the remaining observations\n",
    "2. Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine-affective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (you can create multiple code blocks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recode-task",
   "metadata": {},
   "source": [
    "## Creating the Centrist Variable \n",
    "\n",
    "For our hypothesis, we need to identify \"centrist\" individuals. \n",
    "Therefore, we will should add a new variable capturing this concept into our data frame.\n",
    "\n",
    "**Task:**\n",
    "- Use the `lrscale` to create a centrism variable\n",
    "- Add the new variable `centrism` to the data frame\n",
    "- Check the distribution of the newly created variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-centrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Create the centrism variable\n",
    "\n",
    "\n",
    "# Check the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5869b",
   "metadata": {},
   "source": [
    "## Creating an Affective Scale?\n",
    "\n",
    "For our hypothesis, we also need to build an affective evaluation scale.\n",
    "\n",
    "Unfortunately, ... the ESS does not have direct out-party dislike or feeling thermometers toward parties.\n",
    "Not having direct observations of what we are trying to test, is common when doing research. \n",
    "\n",
    "We need need to find a way around this by building a proxy that is close to our original concept.\n",
    "One way to build such proxy is by creating an additive scale combining multiple variables of interest. \n",
    "\n",
    "That is, we could add variables together to build an \"Affective Scale\".\n",
    "\n",
    "**Task:**\n",
    "- Select some or all trust and satisfaction variables.\n",
    "- Make sure they are coded in the same direction (higher = more positive evaluation).\n",
    "- Combine them into an additive scale (sum or average).\n",
    "- Create a new variable called `aff_eval` in the data frame.\n",
    "- Check the distribution of aff_eval using summary statistics and a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Create the aff_eval variable\n",
    "\n",
    "\n",
    "# Check the distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Now let's explore our cleaned data to understand the relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for key variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crosstabs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstabs and plots to examine the relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-dataset",
   "metadata": {},
   "source": [
    "## Saving the Final Dataset\n",
    "\n",
    "Let's create our final analysis-ready dataset with the key variables for testing our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cleaned dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-spsc-2026 (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
